{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f50e7a-96ee-4f05-b8b4-970e298a7064",
   "metadata": {},
   "source": [
    "# LangChain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd53447-4956-4959-b6a9-28a6446e9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "// import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatOllama } from \"@langchain/community/chat_models/ollama\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"human\", \"Tell me a short joke about {topic}\"],\n",
    "]);\n",
    "// const model = new ChatOpenAI({});\n",
    "const model = new ChatOllama({\n",
    "  baseUrl: \"http://localhost:11434\", // Default value\n",
    "  model: \"qwen\", // Default value\n",
    "  // model: \"llama3\", // Default value\n",
    "  // model: \"datouxia/llama3-8b-chinese-chat-q8-v2\", // Default value\n",
    "});\n",
    "const outputParser = new StringOutputParser();\n",
    "\n",
    "const chain = prompt.pipe(model).pipe(outputParser);\n",
    "\n",
    "const response = await chain.invoke({\n",
    "  topic: \"ice cream\",\n",
    "});\n",
    "console.log(response);\n",
    "/**\n",
    "Why did the ice cream go to the gym?\n",
    "Because it wanted to get a little \"cone\"ditioning!\n",
    " */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254113bb-bc2e-441b-befc-a03faa1e4c36",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "lc_serializable: 这是一个标记字段,表示该对象可以被序列化。\n",
    "\n",
    "lc_kwargs: 这是一个字典,包含了用于实例化该对象的关键字参数。在这个例子中,它包含了一个messages字段,这是一个消息列表,代表对话历史。\n",
    "\n",
    "lc_namespace: 这是一个列表,表示该对象所属的命名空间。在这个例子中,它来自langchain_core.prompt_values模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147a5e8-2b72-41ed-8bc1-8586b77c20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"human\", \"Tell me a short joke about {topic}\"],\n",
    "]);\n",
    "const promptValue = await prompt.invoke({ topic: \"ice cream\" });\n",
    "console.log(promptValue);\n",
    "/**\n",
    "ChatPromptValue {\n",
    "  messages: [\n",
    "    HumanMessage {\n",
    "      content: 'Tell me a short joke about ice cream',\n",
    "      name: undefined,\n",
    "      additional_kwargs: {}\n",
    "    }\n",
    "  ]\n",
    "}\n",
    " */\n",
    "const promptAsMessages = promptValue.toChatMessages();\n",
    "console.log(promptAsMessages);\n",
    "/**\n",
    "[\n",
    "  HumanMessage {\n",
    "    content: 'Tell me a short joke about ice cream',\n",
    "    name: undefined,\n",
    "    additional_kwargs: {}\n",
    "  }\n",
    "]\n",
    " */\n",
    "const promptAsString = promptValue.toString();\n",
    "console.log(promptAsString);\n",
    "/**\n",
    "Human: Tell me a short joke about ice cream\n",
    " */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d9f0db-6367-42d6-95d7-ce4fca7f9bc2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded30bd4-5a9e-48ee-8fa5-7c70147e69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOllama } from \"@langchain/community/chat_models/ollama\";\n",
    "const model = new ChatOllama({\n",
    "  baseUrl: \"http://localhost:11434\", // Default value\n",
    "  model: \"qwen\", // Default value\n",
    "});\n",
    "const promptAsString = \"Human: Tell me a short joke about ice cream\";\n",
    "\n",
    "const response = await model.invoke(promptAsString);\n",
    "console.log(response);\n",
    "/**\n",
    "AIMessage {\n",
    "  content: 'Sure, here you go: Why did the ice cream go to school? Because it wanted to get a little \"sundae\" education!',\n",
    "  name: undefined,\n",
    "  additional_kwargs: { function_call: undefined, tool_calls: undefined }\n",
    "}\n",
    " */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06431884-6af9-4848-8bd8-c7af15799f7a",
   "metadata": {},
   "source": [
    "### If our model was an LLM, it would output a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13b157-3af1-4bb8-8219-89fe37b00e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "\n",
    "const model = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\", // Default value\n",
    "  model: \"qwen\", // Default value\n",
    "});\n",
    "const promptAsString = \"Human: Tell me a short joke about ice cream\";\n",
    "\n",
    "const response = await model.invoke(promptAsString);\n",
    "console.log(response);\n",
    "/**\n",
    "Why did the ice cream go to therapy?\n",
    "\n",
    "Because it was feeling a little rocky road.\n",
    " */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6cd63-db28-41db-a0d3-c85d565ead46",
   "metadata": {},
   "source": [
    "### ChatOllama 和 Ollama 区别\n",
    "ChatOllama 和 Ollama 是两个不同的概念,它们在 LangChain 中具有不同的用途和含义。\n",
    "\n",
    "Ollama\n",
    "\n",
    "Ollama 是一个 LangChain 中的基础模型类,它继承自 BaseModel 类,旨在为语言模型提供统一的接口。Ollama 类本身并不包含任何模型实现,而是作为子类化的基类,被其他具体的语言模型类继承和扩展。\n",
    "Ollama 类定义了一些通用的方法,如 generate 用于生成文本,query 用于查询模型,以及一些辅助方法。通过继承 Ollama 类,具体的语言模型实现可以复用这些通用方法,并针对自身的特性进行扩展和定制。\n",
    "\n",
    "ChatOllama\n",
    "\n",
    "ChatOllama 是 LangChain 中一个专门用于对话模型的类,它继承自 Ollama 类。ChatOllama 类针对对话场景进行了优化和扩展,提供了更加适合对话的接口和功能。\n",
    "ChatOllama 类的主要特点包括:\n",
    "\n",
    "支持维护对话历史记录,以便模型能够根据上下文生成响应。\n",
    "提供了 predict 方法,用于生成对话响应。\n",
    "支持设置不同的对话策略,如提示词、开头和结尾等。\n",
    "可以与不同的对话格式和数据结构集成,如 ChatPromptValue、ChatResult 等。\n",
    "\n",
    "总的来说,Ollama 是一个通用的基础模型类,而 ChatOllama 则是针对对话场景的特殊化模型类,两者的关系是继承关系。在构建对话应用程序时,通常会使用 ChatOllama 或其他继承自 ChatOllama 的具体对话模型实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe47216-5e44-44a3-be7b-2291bf2f7311",
   "metadata": {},
   "source": [
    "## RAG Search Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80375d67-cbe6-4b49-a93f-ae808eeb4477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where did Harrison work?\n",
      "[ Document { pageContent: \u001b[32m\"Harrison worked at Kensho\"\u001b[39m, metadata: {} } ]\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOllama\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"AIMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Answer the question based on only the following context:\\n  \\nHarrison worked at Kensho\",\n",
      "          \"tool_calls\": [],\n",
      "          \"invalid_tool_calls\": [],\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Where did Harrison work?\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOllama\u001b[22m\u001b[39m] [4.30s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Harrison worked at Kensho.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Harrison worked at Kensho.\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "Harrison worked at Kensho.\n"
     ]
    }
   ],
   "source": [
    "// import { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "import { OllamaEmbeddings } from \"@langchain/community/embeddings/ollama\";\n",
    "import { ChatOllama } from \"@langchain/community/chat_models/ollama\";\n",
    "// import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\n",
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "import { Document } from \"@langchain/core/documents\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import {\n",
    "  RunnableLambda,\n",
    "  RunnableMap,\n",
    "  RunnablePassthrough,\n",
    "} from \"@langchain/core/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "  [\n",
    "    new Document({ pageContent: \"Harrison worked at Kensho\" }),\n",
    "    new Document({ pageContent: \"Bears like to eat honey.\" }),\n",
    "  ],\n",
    "  new OllamaEmbeddings({\n",
    "    model: \"nomic-embed-text\", // 成功\n",
    "    // model: \"mxbai-embed-large\", // 成功\n",
    "    // model: \"snowflake-arctic-embed\", // 失败\n",
    "    baseUrl: \"http://localhost:11434\", // default value\n",
    "  })\n",
    ");\n",
    "const retriever = vectorStore.asRetriever(1);\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"ai\",\n",
    "    `Answer the question based on only the following context:\n",
    "  \n",
    "{context}`,\n",
    "  ],\n",
    "  [\"human\", \"{question}\"],\n",
    "]);\n",
    "// const model = new ChatOpenAI({});\n",
    "const model = new ChatOllama({\n",
    "  baseUrl: \"http://localhost:11434\", // Default value\n",
    "  model: \"qwen\", // Default value\n",
    "  verbose: true,\n",
    "});\n",
    "const outputParser = new StringOutputParser();\n",
    "\n",
    "const setupAndRetrieval = RunnableMap.from({\n",
    "  context: new RunnableLambda({\n",
    "    func: (input: string) => {\n",
    "        console.log(input)\n",
    "      return retriever.invoke(input).then((response) => {\n",
    "          console.log(response)\n",
    "          return response[0].pageContent\n",
    "      }) \n",
    "    }\n",
    "  }).withConfig({ runName: \"contextRetriever\" }),\n",
    "  question: new RunnablePassthrough(),\n",
    "});\n",
    "const chain = setupAndRetrieval.pipe(prompt).pipe(model).pipe(outputParser);\n",
    "\n",
    "const response = await chain.invoke(\"Where did Harrison work?\");\n",
    "console.log(response);\n",
    "/**\n",
    "Harrison worked at Kensho.\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdee27-75f2-4f42-92ff-6437b52f18ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
